Transformer:
  node_feats: 3
  pos_dim: 2
  n_targets: 3
  n_hidden: 64
  num_feat_layers: 0
  num_encoder_layers: 4
  n_head: 4
  normalizer: False
  dim_feedforward: 128
  residual_type: add
  attention_type: galerkin
  attn_activation: gelu
  feat_extract_type: None
  xavier_init: 0.01
  diagonal_weight: 0.01
  symmetric_init: False
  layer_norm: True
  attn_norm: False
  norm_eps: 0.00001
  batch_norm: False
  return_attn_weight: False
  return_latent: False
  decoder_type: ifft2
  last_activation: True
  freq_dim: 16
  num_regressor_layers: 2
  regressor_activation: gelu
  fourier_modes: 32
  spacial_dim: 2
  spacial_fc: False
  dropout: 0.05
  encoder_dropout: 0.0
  decoder_dropout: 0.0
  ffn_dropout: 0.05
  upsample_mode: interp
  downsample_mode: interp